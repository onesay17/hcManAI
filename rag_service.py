"""
RAG ì„œë¹„ìŠ¤ ëª¨ë“ˆ
ì„ë² ë”© ìƒì„±, Vector DB ê²€ìƒ‰, SQL ìƒì„± íŒŒì´í”„ë¼ì¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
"""
import chromadb
from chromadb.config import Settings as ChromaSettings
from typing import List
from config import settings
from llm_service import LLMService


class RAGService:
    """RAG ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
        self.chroma_client = chromadb.HttpClient(
            host=settings.CHROMA_HOST,
            port=settings.CHROMA_PORT
        )
        
        # ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        try:
            self.collection = self.chroma_client.get_collection(
                name=settings.CHROMA_COLLECTION_NAME
            )
        except Exception:
            # ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±
            self.collection = self.chroma_client.create_collection(
                name=settings.CHROMA_COLLECTION_NAME
            )
        
        # LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.llm_service = LLMService()
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self._init_embedding_model()
    
    def _init_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (Gemini ì‚¬ìš©)"""
        if not settings.GOOGLE_API_KEY:
            raise ValueError("ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ GOOGLE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        from langchain_google_genai import GoogleGenerativeAIEmbeddings
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            google_api_key=settings.GOOGLE_API_KEY
        )
    
    def generate_embedding(self, text: str) -> List[float]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        
        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            
        Returns:
            ì„ë² ë”© ë²¡í„°
        """
        return self.embeddings.embed_query(text)
    
    def search_similar_schemas(self, query: str, top_k: int = None) -> List[str]:
        """
        Vector DBì—ì„œ ìœ ì‚¬í•œ ìŠ¤í‚¤ë§ˆ íŒíŠ¸ ê²€ìƒ‰
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ê°€ì ¸ì˜¬ ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            ìœ ì‚¬í•œ ìŠ¤í‚¤ë§ˆ íŒíŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        if top_k is None:
            top_k = settings.TOP_K_RESULTS
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.generate_embedding(query)
        
        # Vector DBì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ë” ë§ì€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°)
        try:
            # ì»¬ë ‰ì…˜ì˜ ì „ì²´ ë¬¸ì„œ ìˆ˜ í™•ì¸
            collection_count = self.collection.count()
            print(f"ğŸ“Š Vector DB ì»¬ë ‰ì…˜ ì´ ë¬¸ì„œ ìˆ˜: {collection_count}")
            
            # ê²€ìƒ‰í•  ê°œìˆ˜ë¥¼ ì»¬ë ‰ì…˜ í¬ê¸°ì— ë§ê²Œ ì¡°ì •
            search_k = min(top_k * 2, max(collection_count, top_k))
            
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=search_k
            )
            
            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            documents = results.get('documents', [])
            print(f"ğŸ” Vector DB ê²€ìƒ‰ ê²°ê³¼: {len(documents)}ê°œ ë¬¸ì„œ ë°œê²¬")
            
            if documents and len(documents) > 0:
                found_docs = documents[0]  # ì²« ë²ˆì§¸ ì¿¼ë¦¬ì˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
                print(f"ğŸ“„ ê²€ìƒ‰ëœ ìŠ¤í‚¤ë§ˆ íŒíŠ¸ ê°œìˆ˜: {len(found_docs)}")
                if len(found_docs) > 0:
                    print(f"ğŸ“ ì²« ë²ˆì§¸ íŒíŠ¸ ë¯¸ë¦¬ë³´ê¸°: {found_docs[0][:300]}...")
                    # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°˜í™˜ (ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ì œê³µ)
                    return found_docs
            else:
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì „ì²´ ì»¬ë ‰ì…˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
                print("âš ï¸  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ì „ì²´ ìŠ¤í‚¤ë§ˆë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.")
                all_results = self.collection.get()
                if all_results and 'documents' in all_results:
                    all_docs = all_results['documents']
                    print(f"ğŸ“š ì „ì²´ ìŠ¤í‚¤ë§ˆ ë¬¸ì„œ ìˆ˜: {len(all_docs)}")
                    return all_docs[:top_k] if all_docs else []
        except Exception as e:
            print(f"âŒ Vector DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì „ì²´ ì»¬ë ‰ì…˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            try:
                all_results = self.collection.get()
                if all_results and 'documents' in all_results:
                    all_docs = all_results['documents']
                    print(f"ğŸ“š ì „ì²´ ìŠ¤í‚¤ë§ˆ ë¬¸ì„œ ìˆ˜: {len(all_docs)}")
                    return all_docs[:top_k] if all_docs else []
            except Exception as e2:
                print(f"âŒ ì „ì²´ ìŠ¤í‚¤ë§ˆ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e2}")
        
        print("âš ï¸  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    def generate_sql(self, query: str) -> str:
        """
        RAG íŒŒì´í”„ë¼ì¸ì„ í†µí•´ SQL ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            ìƒì„±ëœ SQL ì¿¼ë¦¬
        """
        print(f"ğŸ“¥ ì‚¬ìš©ì ì§ˆë¬¸: {query}")
        
        # 1. Vector DBì—ì„œ ìœ ì‚¬í•œ ìŠ¤í‚¤ë§ˆ íŒíŠ¸ ê²€ìƒ‰
        similar_schemas = self.search_similar_schemas(query)
        
        # 2. ìŠ¤í‚¤ë§ˆ íŒíŠ¸ ì¡°í•©
        schema_hints = "\n\n".join(similar_schemas) if similar_schemas else "ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        print(f"ğŸ“‹ ì „ë‹¬ëœ ìŠ¤í‚¤ë§ˆ íŒíŠ¸ ê¸¸ì´: {len(schema_hints)} ë¬¸ì")
        if len(schema_hints) > 500:
            print(f"ğŸ“‹ ìŠ¤í‚¤ë§ˆ íŒíŠ¸ ë¯¸ë¦¬ë³´ê¸°: {schema_hints[:500]}...")
        else:
            print(f"ğŸ“‹ ìŠ¤í‚¤ë§ˆ íŒíŠ¸: {schema_hints}")
        
        # 3. LLMì— SQL ìƒì„± ìš”ì²­
        sql = self.llm_service.generate_sql(query, schema_hints)
        print(f"âœ… ìƒì„±ëœ SQL: {sql}")
        
        return sql

